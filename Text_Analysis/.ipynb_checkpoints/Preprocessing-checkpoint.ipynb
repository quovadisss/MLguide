{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['As it continues to spread around the world, bringing panic with it, scientists are striving to develop ways of fighting this previously unknown threat.', 'Sophisticated computer modelling is being used to track and predict its transmission, while virologists are attempting to engineer a vaccine.', 'Others are seeking drug treatments that can help those who fall ill with the infection.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'As it continues to spread around the world, bringing panic with it, scientists are striving to develop ways of fighting this previously unknown threat. \\\n",
    "Sophisticated computer modelling is being used to track and predict its transmission, while virologists are attempting to engineer a vaccine. \\\n",
    "Others are seeking drug treatments that can help those who fall ill with the infection.'\n",
    "sentences = sent_tokenize(text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'it', 'continues', 'to', 'spread', 'around', 'the', 'world', ',', 'bringing', 'panic', 'with', 'it', ',', 'scientists', 'are', 'striving', 'to', 'develop', 'ways', 'of', 'fighting', 'this', 'previously', 'unknown', 'threat', '.', 'Sophisticated', 'computer', 'modelling', 'is', 'being', 'used', 'to', 'track', 'and', 'predict', 'its', 'transmission', ',', 'while', 'virologists', 'are', 'attempting', 'to', 'engineer', 'a', 'vaccine', '.', 'Others', 'are', 'seeking', 'drug', 'treatments', 'that', 'can', 'help', 'those', 'who', 'fall', 'ill', 'with', 'the', 'infection', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text_sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['As', 'it', 'continues', 'to', 'spread', 'around', 'the', 'world', ',', 'bringing', 'panic', 'with', 'it', ',', 'scientists', 'are', 'striving', 'to', 'develop', 'ways', 'of', 'fighting', 'this', 'previously', 'unknown', 'threat', '.'], ['Sophisticated', 'computer', 'modelling', 'is', 'being', 'used', 'to', 'track', 'and', 'predict', 'its', 'transmission', ',', 'while', 'virologists', 'are', 'attempting', 'to', 'engineer', 'a', 'vaccine', '.'], ['Others', 'are', 'seeking', 'drug', 'treatments', 'that', 'can', 'help', 'those', 'who', 'fall', 'ill', 'with', 'the', 'infection', '.']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['continues', 'spread', 'around', 'world', ',', 'bringing', 'panic', ',', 'scientists', 'striving', 'develop', 'ways', 'fighting', 'previously', 'unknown', 'threat', '.'], ['sophisticated', 'computer', 'modelling', 'used', 'track', 'predict', 'transmission', ',', 'virologists', 'attempting', 'engineer', 'vaccine', '.'], ['others', 'seeking', 'drug', 'treatments', 'help', 'fall', 'ill', 'infection', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amus\n",
      "amuse\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem('amusing'))\n",
    "print(lemma.lemmatize('amusing', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
