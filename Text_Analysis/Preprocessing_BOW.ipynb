{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from scipy import sparse\n",
    "import nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['As it continues to spread around the world, bringing panic with it, scientists are striving to develop ways of fighting this previously unknown threat.', 'Sophisticated computer modelling is being used to track and predict its transmission, while virologists are attempting to engineer a vaccine.', 'Others are seeking drug treatments that can help those who fall ill with the infection.']\n"
     ]
    }
   ],
   "source": [
    "text_sample = 'As it continues to spread around the world, bringing panic with it, scientists are striving to develop ways of fighting this previously unknown threat. \\\n",
    "Sophisticated computer modelling is being used to track and predict its transmission, while virologists are attempting to engineer a vaccine. \\\n",
    "Others are seeking drug treatments that can help those who fall ill with the infection.'\n",
    "sentences = sent_tokenize(text_sample)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['As', 'it', 'continues', 'to', 'spread', 'around', 'the', 'world', ',', 'bringing', 'panic', 'with', 'it', ',', 'scientists', 'are', 'striving', 'to', 'develop', 'ways', 'of', 'fighting', 'this', 'previously', 'unknown', 'threat', '.', 'Sophisticated', 'computer', 'modelling', 'is', 'being', 'used', 'to', 'track', 'and', 'predict', 'its', 'transmission', ',', 'while', 'virologists', 'are', 'attempting', 'to', 'engineer', 'a', 'vaccine', '.', 'Others', 'are', 'seeking', 'drug', 'treatments', 'that', 'can', 'help', 'those', 'who', 'fall', 'ill', 'with', 'the', 'infection', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text_sample)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['As', 'it', 'continues', 'to', 'spread', 'around', 'the', 'world', ',', 'bringing', 'panic', 'with', 'it', ',', 'scientists', 'are', 'striving', 'to', 'develop', 'ways', 'of', 'fighting', 'this', 'previously', 'unknown', 'threat', '.'], ['Sophisticated', 'computer', 'modelling', 'is', 'being', 'used', 'to', 'track', 'and', 'predict', 'its', 'transmission', ',', 'while', 'virologists', 'are', 'attempting', 'to', 'engineer', 'a', 'vaccine', '.'], ['Others', 'are', 'seeking', 'drug', 'treatments', 'that', 'can', 'help', 'those', 'who', 'fall', 'ill', 'with', 'the', 'infection', '.']]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return word_tokens\n",
    "\n",
    "word_tokens = tokenize_text(text_sample)\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['continues', 'spread', 'around', 'world', ',', 'bringing', 'panic', ',', 'scientists', 'striving', 'develop', 'ways', 'fighting', 'previously', 'unknown', 'threat', '.'], ['sophisticated', 'computer', 'modelling', 'used', 'track', 'predict', 'transmission', ',', 'virologists', 'attempting', 'engineer', 'vaccine', '.'], ['others', 'seeking', 'drug', 'treatments', 'help', 'fall', 'ill', 'infection', '.']]\n"
     ]
    }
   ],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Stemming, Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amus\n",
      "amuse\n"
     ]
    }
   ],
   "source": [
    "stemmer = LancasterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "print(stemmer.stem('amusing'))\n",
    "print(lemma.lemmatize('amusing', 'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 BOW를 하면 sparse matrix형식을 띄게 되는데 물리적으로 적은 메모리 공간을 차지할 수 있도록 변환 해주는 방법이 있다. <br>\n",
    "1. COO <br>\n",
    "2. CSR (보통 이 방법을 많이 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COO\n",
    "dense = np.array([[3, 0, 1], [0, 2, 0]])\n",
    "\n",
    "# 0이 아닌 데이터\n",
    "data = np.array([3, 1, 2])\n",
    "\n",
    "# 행 위ㅣ와 열 위치를 각각 배열로 생성\n",
    "row_pos = np.array([0, 0, 1])\n",
    "col_pos = np.array([0, 2, 1])\n",
    "\n",
    "# COO\n",
    "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "### CPR\n",
    "# 행 위치에 대한 index를 다시 나타내서 메모리를 줄인다.\n",
    "dense2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "# COO\n",
    "sparse_coo = sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
    "\n",
    "# 행 위치 배열의 고유한 값의 시작 위치 인덱스. 마지막 숫자는 행 위치 배열의 크기\n",
    "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
    "\n",
    "# CSR\n",
    "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
    "\n",
    "print(sparse_coo.toarray())\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be simple\n",
    "coo = sparse.coo_matrix(dense2)\n",
    "csr = sparse.csr_matrix(dense2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
